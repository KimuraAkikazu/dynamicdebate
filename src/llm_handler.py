
"""llama-cpp-python „Çí„É©„ÉÉ„Éó„Åô„Çã„Ç∑„É≥„Ç∞„É´„Éà„É≥ LLMHandlerÔºà„Ç®„Éº„Ç∏„Çß„É≥„ÉàÂêçÂØæÂøúÁâàÔºâ"""
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Optional, Sequence

from llama_cpp import Llama
from . import prompts
from .prompt_logger import PromptLogger


class LLMHandler:
    _instance: "LLMHandler" | None = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        config: Dict[str, Any],
        *,
        prompt_logger: Optional[PromptLogger] = None,
    ) -> None:
        if hasattr(self, "_initialized") and self._initialized:
            return
        self._initialized = True

        self.logger = prompt_logger
        self.model_path = Path(__file__).resolve().parents[1] / "models" / config["filename"]
        if not self.model_path.exists():
            raise FileNotFoundError(f"„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {self.model_path}")

        print(f"[LLMHandler] üîÑ „É¢„Éá„É´Ë™≠„ÅøËæº„ÅøÈñãÂßã: {self.model_path}")
        self.model = Llama(
            model_path=str(self.model_path),
            n_gpu_layers=config.get("n_gpu_layers", -1),
            n_ctx=config.get("n_ctx", 4096),
            temperature=config.get("temperature", 0.7),
            max_tokens=config.get("max_tokens", 512),
            chat_format="llama-3",
        )
        print("[LLMHandler] ‚úÖ „É¢„Éá„É´Ë™≠„ÅøËæº„ÅøÂÆå‰∫Ü")

    # ======================  ÂÜÖÈÉ®: „Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„ÉàÊßãÁØâ  ====================== #
    def _build_system_prompt(
        self,
        max_turn: int,
        turn: int,
        *,
        name: str = "„ÅÇ„Å™„Åü",
        peer_names: Optional[Sequence[str]] = None,
    ) -> str:
        """Assemble system prompt string with per-agent names.

        Only the first two peer names are used; missing names are auto-filled.
        """
        p1 = p2 = "‰ªñ„ÅÆ„Ç®„Éº„Ç∏„Çß„É≥„Éà"
        if peer_names:
            if len(peer_names) >= 1:
                p1 = peer_names[0]
            if len(peer_names) >= 2:
                p2 = peer_names[1]
            if len(peer_names) > 2:
                # collapse extras into p2 for debugging visibility
                p2 = "„Éª".join(peer_names[1:])

        return prompts.SYSTEM_PROMPT.format(
            name=name,
            peer1=p1,
            peer2=p2,
            max_turn=max_turn,
            turn=turn,
            turns_left=max_turn - turn,
        )

    # ======================  ÂÖ¨Èñã API  ====================== #
    def generate_action(
        self,
        user_prompt: str,
        turn: int,
        max_turn: int,
        *,
        agent_name: str | None = None,
        peer_names: Optional[Sequence[str]] = None,
    ) -> Dict[str, Any]:
        phase = "plan"
        system_prompt = self._build_system_prompt(
            max_turn, turn, name=agent_name or "„ÅÇ„Å™„Åü", peer_names=peer_names
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        if self.logger and agent_name:
            self.logger.log(agent_name, phase, turn, system_prompt, user_prompt)

        try:
            resp = self.model.create_chat_completion(
                messages=messages,
                response_format={"type": "json_object"},
            )
            raw = resp["choices"][0]["message"]["content"]
            return json.loads(raw)
        except Exception:
            return {"action": "listen", "thought": "JSON Ëß£ÊûêÂ§±Êïó‚Üílisten"}

    def generate_utterance(
        self,
        user_prompt: str,
        turn: int,
        max_turn: int,
        *,
        agent_name: str | None = None,
        peer_names: Optional[Sequence[str]] = None,
    ) -> str:
        phase = "utterance"
        system_prompt = self._build_system_prompt(
            max_turn, turn, name=agent_name or "„ÅÇ„Å™„Åü", peer_names=peer_names
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        if self.logger and agent_name:
            self.logger.log(agent_name, phase, turn, system_prompt, user_prompt)

        try:
            resp = self.model.create_chat_completion(messages=messages)
            print(resp["choices"][0]["message"]["content"])
            return resp["choices"][0]["message"]["content"].strip()
        except Exception:
            return "‚Ä¶"
