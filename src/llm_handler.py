"""llama-cpp-python ã‚’ãƒ©ãƒƒãƒ—ã™ã‚‹ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ LLMHandler"""
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Optional

from llama_cpp import Llama
from . import prompts
from .prompt_logger import PromptLogger


class LLMHandler:
    """ãƒ¢ãƒ‡ãƒ«ã‚’ 1 åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ã—ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§å…±æœ‰ã™ã‚‹"""

    _instance: "LLMHandler" | None = None  # ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ä¿æŒç”¨

    # --------------------------------------------------------------------- #
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    # --------------------------------------------------------------------- #
    def __init__(
        self,
        config: Dict[str, Any],
        *,
        prompt_logger: Optional[PromptLogger] = None,
    ) -> None:
        if hasattr(self, "_initialized") and self._initialized:  # 2 å›ç›®ä»¥é™ã¯ã‚¹ã‚­ãƒƒãƒ—
            return
        self._initialized = True

        # ãƒ­ã‚¬ãƒ¼
        self.logger = prompt_logger

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        self.model_path = Path(__file__).resolve().parents[1] / "models" / config["filename"]
        if not self.model_path.exists():
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_path}")

        print(f"[LLMHandler] ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {self.model_path}")
        self.model = Llama(
            model_path=str(self.model_path),
            n_gpu_layers=config.get("n_gpu_layers", -1),
            n_ctx=config.get("n_ctx", 4096),
            temperature=config.get("temperature", 0.7),
            max_tokens=config.get("max_tokens", 512),
            chat_format="llama-3",
        )
        print("[LLMHandler] âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

    # ======================  å…¬é–‹ API  ====================== #
    def generate_action(
        self,
        user_prompt: str,
        turn: int,
        max_turn: int,
        *,
        agent_name: str | None = None,
    ) -> Dict[str, Any]:
        """JSON å½¢å¼ã®è¡Œå‹•è¨ˆç”»ã‚’è¿”ã™"""
        phase = "plan"
        system_prompt = prompts.SYSTEM_PROMPT.format(max_turn=max_turn, turn=turn)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # ãƒ­ã‚°ä¿å­˜
        if self.logger and agent_name:
            self.logger.log(agent_name, phase, turn, system_prompt, user_prompt)

        try:
            resp = self.model.create_chat_completion(
                messages=messages,
                response_format={"type": "json_object"},
            )
            raw = resp["choices"][0]["message"]["content"]
            return json.loads(raw)
        except Exception as e:  # JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã‚„æ¨è«–ã‚¨ãƒ©ãƒ¼
            print(f"[LLMHandler] âš ï¸ generate_action å¤±æ•—: {e}")
            return {"action": "listen", "thought": "JSON è§£æã«å¤±æ•—ã—ãŸã®ã§èãæ‰‹ã«å›ã‚‹"}

    # ------------------------------------------------------ #
    def generate_utterance(
        self,
        user_prompt: str,
        turn: int,
        max_turn: int,
        *,
        agent_name: str | None = None,
    ) -> str:
        """è‡ªç„¶è¨€èªã®ç™ºè©±å…¨æ–‡ã‚’è¿”ã™"""
        phase = "utterance"
        system_prompt = prompts.SYSTEM_PROMPT.format(max_turn=max_turn, turn=turn)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # ãƒ­ã‚°ä¿å­˜
        if self.logger and agent_name:
            self.logger.log(agent_name, phase, turn, system_prompt, user_prompt)

        try:
            resp = self.model.create_chat_completion(messages=messages)
            return resp["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"[LLMHandler] âš ï¸ generate_utterance å¤±æ•—: {e}")
            return "â€¦"
